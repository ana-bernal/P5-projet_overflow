{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ùìüë• Tag suggestion 4: testing API code in a notebook\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "| Project  |    [Cat√©gorisez automatiquement des questions](#https://openclassrooms.com/fr/paths/148/projects/111/assignment)         |\n",
    "| Date   |   March 2023   |\n",
    "| Autor  | Ana Bernal                                                    |\n",
    "| Data source | [StackExchange Data Explorer](https://data.stackexchange.com/stackoverflow/query/new) |\n",
    "| Mentor | Samir Tanfous | \n",
    "| Notebook number  | 4 of 4                                                    |\n",
    "\n",
    "**Description:** In this notebook we load the trained model and write the code for an API which suggests a set of tags from a text input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of contents:\n",
    "- [Libraries](#Libraries)\n",
    "- [Loading files](#Loading-files)\n",
    "- [Parameters](#Parameters)\n",
    "- [Functions](#Functions)\n",
    "- [Examples](#Examples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 17:33:12.592440: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 17:33:13.216351: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:33:13.216380: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-02 17:33:15.013030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:33:15.013893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:33:15.013919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-02 17:33:19.114704: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:33:19.115192: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-02 17:33:19.115220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nemo): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# For loading files\n",
    "from joblib import dump, load\n",
    "\n",
    "# Model hub\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Language/text\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "# for listing tags from binary sequence\n",
    "from itertools import compress"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './trained_models/'\n",
    "filename_model = 'multinomialNB-use.joblib'\n",
    "filename_scaler = 'scaler.joblib'\n",
    "\n",
    "# Loading model\n",
    "clf = load(path + filename_model)\n",
    "\n",
    "# Loading scaler\n",
    "scaler = load(path + filename_scaler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.4\n",
    "tag_list = ['c#', \n",
    "            'java', \n",
    "            'javascript', \n",
    "            'python', \n",
    "            'c++', \n",
    "            'ios', \n",
    "            'android', \n",
    "            '.net', \n",
    "            'html', \n",
    "            'php', \n",
    "            'objective-c', \n",
    "            'jquery', \n",
    "            'c', \n",
    "            'iphone', \n",
    "            'sql', \n",
    "            'asp.net', \n",
    "            'css', \n",
    "            'linux', \n",
    "            'node.js', \n",
    "            'performance', \n",
    "            'spring', \n",
    "            'windows', \n",
    "            'swift', \n",
    "            'xcode', \n",
    "            'ruby-on-rails', \n",
    "            'mysql', \n",
    "            'json', \n",
    "            'sql-server', \n",
    "            'multithreading', \n",
    "            'asp.net-mvc', \n",
    "            'ruby', \n",
    "            'database', \n",
    "            'wpf', \n",
    "            'unit-testing', \n",
    "            'macos', \n",
    "            'arrays', \n",
    "            'c++11', \n",
    "            'django']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_code(text):\n",
    "    \"\"\"\n",
    "    Removes \"<code> some text </code>\" from a text.\n",
    "    or \"<script> some text </script>\"\n",
    "\n",
    "    Parameters\n",
    "        - text : str\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text,'lxml')\n",
    "    code_to_remove = soup.findAll('code')\n",
    "    for code in code_to_remove:\n",
    "        code.replace_with(' ')\n",
    "        \n",
    "    code_to_remove = soup.findAll('script')\n",
    "    for code in code_to_remove:\n",
    "        code.replace_with(' ')\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "def instantiate_spacy():\n",
    "    global nlp\n",
    "    # Instantiating language model, english\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def import_stopwords():\n",
    "    # Importing stopwords\n",
    "    with open('./stopwords/stopwords.txt') as file:\n",
    "        my_stopwords = {line.rstrip() for line in file}\n",
    "    \n",
    "    # Adding my_stopwords to spacy stopwords\n",
    "    nlp.Defaults.stop_words = nlp.Defaults.stop_words.union(my_stopwords)\n",
    "\n",
    "def clean(text,tokenize=False,strict=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a dictionnary with keys 'text' or 'tokens', where\n",
    "    'tokens' corresponds tothe list of lemmatized tokens from\n",
    "    the string text. Ommiting stopwords and punctuation, and the text is\n",
    "    the joint text.\n",
    "\n",
    "    Parameters:\n",
    "        - text: str\n",
    "        - tokenize: bool\n",
    "            If True returns list of tokens, if False returns string.\n",
    "        - strict: bool\n",
    "            If true only keeps nouns\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Removing <code>some code</code>\n",
    "    clean_txt = remove_code(text)\n",
    "\n",
    "    # Removing HTML tags\n",
    "    soup = BeautifulSoup(clean_txt, features='html.parser')\n",
    "    clean_txt = soup.get_text()\n",
    "\n",
    "    # Removing new line character: \\n\n",
    "    clean_txt = clean_txt.replace('\\n', ' ')\n",
    "\n",
    "    # Removing unicode characters\n",
    "    clean_txt = clean_txt.encode(\"ascii\", \"ignore\").decode()\n",
    "    \n",
    "    # Removing digits\n",
    "    clean_txt = ''.join(char for char in clean_txt if not char.isdigit())\n",
    "\n",
    "    # Replacing 'c ++' and 'c #' for 'c++' and 'c#' and others\n",
    "    clean_txt = clean_txt.replace('c ++', 'c++')\n",
    "    clean_txt = clean_txt.replace('c #', 'c#')\n",
    "    clean_txt = clean_txt.replace('C ++', 'c++')\n",
    "    clean_txt = clean_txt.replace('C #', 'c#')\n",
    "    clean_txt = clean_txt.replace('C#', 'c#')\n",
    "    clean_txt = clean_txt.replace('C ++', 'c++')\n",
    "\n",
    "    # Adding special case rule\n",
    "    special_case = [{ORTH: \"c#\"}] \n",
    "    nlp.tokenizer.add_special_case(\"c#\", special_case)\n",
    "    special_case = [{ORTH: \".net\"}] \n",
    "    nlp.tokenizer.add_special_case(\".net\", special_case)\n",
    "    special_case = [{ORTH: \"objective-c\"}] \n",
    "    nlp.tokenizer.add_special_case(\"objective-c\", special_case)\n",
    "    special_case = [{ORTH: \"asp.net\"}]\n",
    "    nlp.tokenizer.add_special_case(\"asp.net\", special_case)\n",
    "    special_case = [{ORTH: \"node.js\"}]\n",
    "    nlp.tokenizer.add_special_case(\"node.js\", special_case)\n",
    "    special_case = [{ORTH: \"ruby-on-rails\"}]\n",
    "    nlp.tokenizer.add_special_case(\"ruby-on-rails\", special_case)    \n",
    "    special_case = [{ORTH: \"sql-server\"}] \n",
    "    nlp.tokenizer.add_special_case(\"sql-server\", special_case)    \n",
    "    special_case = [{ORTH: \"unit-testing\"}] \n",
    "    nlp.tokenizer.add_special_case(\"unit-testing\", special_case)\n",
    "  \n",
    "    # Tokenize with spacy\n",
    "    doc = nlp(clean_txt)\n",
    "\n",
    "    # Tokenize properties\n",
    "    if strict == True:\n",
    "        tokens = [token.lemma_.lower() for token in doc\n",
    "                    if token.pos_ in ['NOUN', 'PROPN', 'VERB'] and\n",
    "                        (not (token.is_stop or \n",
    "                              token.is_punct or\n",
    "                              token.is_space\n",
    "                              )\n",
    "                        )\n",
    "                 ]    \n",
    "    else:\n",
    "        tokens = [token.lemma_.lower() for token in doc\n",
    "                    if not (token.is_stop or \n",
    "                            token.is_punct or\n",
    "                            token.is_space\n",
    "                            )\n",
    "                 ]\n",
    "\n",
    "    clean_txt = ' '.join(tokens)\n",
    "    \n",
    "    # Ask if return text or tokens\n",
    "    if tokenize == True:\n",
    "        result = tokens\n",
    "    else:\n",
    "        result = clean_txt\n",
    "\n",
    "    # Option for list of entities in output\n",
    "    if 'ent' in kwargs:\n",
    "        result = {'output':result, 'ents': doc.ents}\n",
    "\n",
    "    return result\n",
    "\n",
    "def my_pred(X):\n",
    "    \"\"\"\n",
    "    Takes an embedding X obtained from the USE\n",
    "    model, scales it with our scaler first and\n",
    "    returns the prediction of our tag suggestion model in \n",
    "    form of a binary list.\n",
    "    \"\"\"\n",
    "    # Scaling with pre-trained scaler\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Predicting probabilities, using best thresh pre-trained\n",
    "    y_pred_proba = clf.predict_proba(X_scaled)\n",
    "    y_pred = (y_pred_proba > thresh).astype(int).reshape((len(tag_list),))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def binary_to_tag_list(binary):\n",
    "    \"\"\"\n",
    "    Converts a binary list to the list of tags (str).\n",
    "    \"\"\"\n",
    "    fil = [bool(x) for x in list(binary)]\n",
    "    list_tags = list(compress(tag_list,fil))\n",
    "\n",
    "    return list_tags\n",
    "\n",
    "def tag_suggestion(raw_text):\n",
    "    \"\"\"\n",
    "    Returns a list of tags suggested for the question raw_text.\n",
    "    \"\"\"\n",
    "    # Clean text first\n",
    "    clean_text = clean(raw_text)\n",
    "    document = [clean_text]\n",
    "\n",
    "    # Find an embedding of the text with USE \n",
    "    X = embed(document)\n",
    "\n",
    "    # Predict a tag set with our classification model\n",
    "    pred = my_pred(X)\n",
    "\n",
    "    return binary_to_tag_list(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 17:34:25.937966: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "instantiate_spacy()\n",
    "\n",
    "import_stopwords()\n",
    "\n",
    "# Import and insantiate embedding model\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['javascript', 'html', 'jquery', 'css']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input_text = \"Jquery/Javascript Opacity animation with scroll <p>I'm looking to change the opacity on an object (and have the transition be animated) based on a users scroll.\\nexample(http://davegamache.com/)</p>\\n\\n<p>I've searched everywhere\\nlike here, but it ends up pointing me to the waypoints plugin (http://stackoverflow.com/questions/6316757/opacity-based-on-scroll-position)</p>\\n\\n<p>I've implemented the [waypoints][1] plugin and have the object fading once it's higher than 100px. [Using the offet attribute] but would like to basically control the opacity of an object and have the animation be visible like the above example.</p>\\n\\n<p>I've searched all over- this is my last resort.\\nAny help is greatly appreciated.</p>\\n\"\n",
    "tag_suggestion(raw_input_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['javascript', 'html', 'jquery']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input_text = 'Setting cross-domain cookies in Safari <p>I have to call domain A.com (which sets the cookies with http) from domain B.com.\\nAll I do on domain B.com is (javascript): </p>\\n\\n<pre><code>var head = document.getElementsByTagName(\"head\")[0];\\nvar script = document.createElement(\"script\");\\nscript.src = \"A.com/setCookie?cache=1231213123\";\\nhead.appendChild(script);\\n</code></pre>\\n\\n<p>This sets the cookie on A.com on every browser I\\'ve tested, except Safari.\\nAmazingly this works in IE6, even without the P3P headers.</p>\\n\\n<p>Is there any way to make this work in Safari?</p>\\n'\n",
    "tag_suggestion(raw_input_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sql', 'mysql', 'sql-server', 'database', 'django']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input_text = 'Database migrations for SQL Server <p>I need a database migration framework for SQL Server, capable of managing both schema changes and data migrations.</p>\\n\\n<p>I guess I am looking for something similar to django\\'s <a href=\"http://south.aeracode.org/\" rel=\"noreferrer\">South</a> framework here.</p>\\n\\n<p>Given the fact that South is tightly coupled with django\\'s ORM, and the fact that there\\'s so many ORMs for SQL Server I guess having just a generic migration framework, enabling you to write and execute in controlled and sequential manner SQL data/schema change scripts should be sufficient.</p>\\n'\n",
    "tag_suggestion(raw_input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
